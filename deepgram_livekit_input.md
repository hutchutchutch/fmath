# Low-Latency Voice Input with Deepgram and LiveKit

This guide details the implementation of a low-latency voice transcription system using Deepgram for speech-to-text and LiveKit for real-time audio streaming. The system achieves sub-600ms latency from voice detection to transcription display.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     WebRTC Audio      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     WebSocket      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   LiveKit   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  Deepgram   â”‚
â”‚  (Frontend) â”‚                      â”‚   Server    â”‚                  â”‚     API     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                                    â”‚                                 â”‚
       â”‚                                    â”‚                                 â”‚
       â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
       â”‚         SSE Updates        â”‚    Backend     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  Node.js App  â”‚    Transcriptions
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Components

### 1. Frontend (React + LiveKit Client SDK)
- Captures microphone audio using LiveKit
- Displays real-time audio levels with Web Audio API
- Shows transcriptions with latency measurements
- Handles WebRTC connection management

### 2. Backend (Node.js + Express)
- Creates LiveKit access tokens
- Subscribes to LiveKit audio tracks
- Processes audio through Deepgram API
- Implements voice activity detection (VAD)
- Streams transcriptions via Server-Sent Events (SSE)

### 3. External Services
- **LiveKit Cloud**: Handles WebRTC infrastructure
- **Deepgram API**: Provides real-time speech-to-text

## Prerequisites

### API Keys Required
```bash
# .env file
LIVEKIT_URL=wss://your-livekit-url.livekit.cloud
LIVEKIT_API_KEY=your_api_key
LIVEKIT_API_SECRET=your_api_secret
DEEPGRAM_API_KEY=your_deepgram_key
```

### Dependencies

#### Backend
```json
{
  "dependencies": {
    "@deepgram/sdk": "^3.9.1",
    "@livekit/rtc-node": "^0.11.1",
    "livekit-server-sdk": "^2.8.1",
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "dotenv": "^16.3.1"
  }
}
```

#### Frontend
```json
{
  "dependencies": {
    "livekit-client": "^2.8.0",
    "react": "^18.2.0"
  }
}
```

## Implementation Guide

### Step 1: Backend Setup

#### 1.1 Express Server with Environment Configuration
```typescript
// server.ts
import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';

// Load environment variables FIRST
dotenv.config();

// Import routes AFTER env vars are loaded
import { livekitRouter } from './routes/livekit';

const app = express();
const PORT = process.env.PORT || 3001;

app.use(cors());
app.use(express.json());
app.use('/api/livekit', livekitRouter);

app.listen(PORT, () => {
  console.log(`ðŸš€ Backend running on port ${PORT}`);
});
```

#### 1.2 LiveKit Token Generation
```typescript
// routes/livekit.ts
import { Router } from 'express';
import { AccessToken } from 'livekit-server-sdk';

const router = Router();

router.post('/token', async (req, res) => {
  const { roomName, participantName } = req.body;
  
  const token = new AccessToken(
    process.env.LIVEKIT_API_KEY!,
    process.env.LIVEKIT_API_SECRET!,
    {
      identity: participantName,
      name: participantName,
    }
  );
  
  token.addGrant({
    roomJoin: true,
    room: roomName,
    canPublish: true,
    canSubscribe: true,
  });

  const jwt = await token.toJwt();
  
  res.json({
    token: jwt,
    url: process.env.LIVEKIT_URL
  });
});
```

#### 1.3 Deepgram Service with Optimizations
```typescript
// services/deepgramService.ts
import { createClient, LiveTranscriptionEvents } from '@deepgram/sdk';
import EventEmitter from 'events';

class DeepgramService extends EventEmitter {
  private deepgram: any;
  private connection: any = null;
  private keepAliveInterval: NodeJS.Timeout | null = null;

  constructor() {
    super();
    const apiKey = process.env.DEEPGRAM_API_KEY;
    if (!apiKey) {
      throw new Error('DEEPGRAM_API_KEY is required');
    }
    this.deepgram = createClient(apiKey);
  }

  async startTranscription() {
    try {
      // Optimized configuration for low latency
      this.connection = this.deepgram.listen.live({
        model: 'nova',  // Fast, accurate model
        language: 'en-US',
        encoding: 'linear16',
        sample_rate: 48000,
        channels: 1,
        interim_results: true,  // Enable for faster feedback
      });

      this.connection.on(LiveTranscriptionEvents.Open, () => {
        console.log('ðŸŽ™ï¸ Deepgram connection opened');
        
        // Keep connection alive to prevent delays
        this.keepAliveInterval = setInterval(() => {
          if (this.connection?.getReadyState() === 1) {
            this.connection.keepAlive();
          }
        }, 3000);
      });

      this.connection.on(LiveTranscriptionEvents.Transcript, (data: any) => {
        const transcript = data.channel?.alternatives?.[0]?.transcript;
        const isFinal = data.is_final;
        
        if (transcript?.trim()) {
          this.emit(isFinal ? 'transcription' : 'interim', transcript);
        }
      });

      return this.connection;
    } catch (error) {
      console.error('Failed to start Deepgram:', error);
      throw error;
    }
  }

  sendAudio(audioData: Buffer) {
    if (this.connection?.getReadyState() === 1) {
      this.connection.send(audioData);
    }
  }

  stop() {
    if (this.keepAliveInterval) {
      clearInterval(this.keepAliveInterval);
    }
    this.connection?.finish();
  }
}
```

#### 1.4 Audio Handler with Voice Activity Detection
```typescript
// services/audioHandler.ts
import { Room, RoomEvent, RemoteAudioTrack, AudioStream } from '@livekit/rtc-node';
import EventEmitter from 'events';

class AudioHandler extends EventEmitter {
  private room: Room | null = null;
  private isSpeaking: boolean = false;
  private speechStartTime: number = 0;
  private silenceCount: number = 0;

  async joinRoom(roomName: string): Promise<void> {
    this.room = new Room();
    
    this.room.on(RoomEvent.TrackSubscribed, async (track, publication, participant) => {
      if (track.kind === TrackKind.KIND_AUDIO) {
        const audioTrack = track as RemoteAudioTrack;
        await this.processAudioTrack(audioTrack, participant.identity);
      }
    });

    // Connect to room
    const token = await this.createBackendToken(roomName);
    await this.room.connect(process.env.LIVEKIT_URL!, token);
  }

  private async processAudioTrack(audioTrack: RemoteAudioTrack, participantId: string) {
    // Start Deepgram
    await deepgramService.startTranscription();
    
    // Listen for transcriptions
    deepgramService.on('transcription', (transcript: string) => {
      const latency = Date.now() - this.speechStartTime;
      this.emit('transcription', {
        text: transcript,
        latency,
        participantId,
        isFinal: true,
      });
    });

    // Create audio stream
    const stream = new AudioStream(audioTrack, {
      sampleRate: 48000,
      numChannels: 1
    });

    const reader = stream.getReader();
    const buffers: Buffer[] = [];

    // Process audio frames in 10ms chunks for low latency
    while (true) {
      const { done, value: frame } = await reader.read();
      if (done) break;
      
      if (!frame?.data) continue;
      
      const buffer = Buffer.from(frame.data);
      buffers.push(buffer);
      
      // Process every 10ms (960 bytes at 48kHz, 16-bit)
      const targetSize = 960;
      const totalSize = buffers.reduce((sum, buf) => sum + buf.length, 0);
      
      if (totalSize >= targetSize) {
        const chunk = Buffer.concat(buffers).slice(0, targetSize);
        buffers.length = 0;
        
        // Voice Activity Detection
        const hasVoice = this.detectVoiceActivity(chunk);
        
        if (hasVoice && !this.isSpeaking) {
          this.isSpeaking = true;
          this.speechStartTime = Date.now();
          console.log(`ðŸŽ¤ Speech detected at ${this.speechStartTime}`);
        } else if (!hasVoice && this.isSpeaking) {
          this.silenceCount++;
          if (this.silenceCount > 2) { // 20ms of silence
            this.isSpeaking = false;
            this.silenceCount = 0;
            console.log(`ðŸ”‡ Speech ended`);
          }
        }
        
        // Send audio to Deepgram
        deepgramService.sendAudio(chunk);
      }
    }
  }

  private detectVoiceActivity(buffer: Buffer): boolean {
    // Calculate RMS energy
    let sum = 0;
    const samples = buffer.length / 2;
    
    for (let i = 0; i < buffer.length; i += 2) {
      const sample = buffer.readInt16LE(i);
      sum += sample * sample;
    }
    
    const rms = Math.sqrt(sum / samples);
    return rms > 500; // Threshold tuned for speech
  }
}
```

### Step 2: Frontend Implementation

#### 2.1 Voice Transcription Component
```tsx
// components/VoiceTranscription.tsx
import React, { useState, useRef, useEffect } from 'react';
import { Room, RoomEvent, createLocalTracks } from 'livekit-client';

interface Transcription {
  text: string;
  latency: number;
  timestamp: number;
  isFinal?: boolean;
}

export const VoiceTranscription: React.FC = () => {
  const [status, setStatus] = useState('Disconnected');
  const [transcriptions, setTranscriptions] = useState<Transcription[]>([]);
  const [audioLevel, setAudioLevel] = useState(0);
  
  const roomRef = useRef<Room | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);

  const setupAudioLevelMonitoring = (mediaStreamTrack: MediaStreamTrack) => {
    // Create Web Audio API analyzer
    const audioContext = new AudioContext();
    const analyser = audioContext.createAnalyser();
    analyser.fftSize = 256;
    
    const source = audioContext.createMediaStreamSource(
      new MediaStream([mediaStreamTrack])
    );
    source.connect(analyser);
    
    // Monitor levels
    const dataArray = new Uint8Array(analyser.frequencyBinCount);
    
    const checkLevel = () => {
      analyser.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
      setAudioLevel(Math.round((average / 255) * 100));
      requestAnimationFrame(checkLevel);
    };
    
    checkLevel();
  };

  const connectToLiveKit = async () => {
    try {
      // Get token from backend
      const roomName = `room-${Date.now()}`;
      const response = await fetch('http://localhost:3001/api/livekit/token', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ roomName, participantName: 'user' })
      });
      
      const { token, url } = await response.json();
      
      // Connect to LiveKit
      const room = new Room();
      await room.connect(url, token);
      roomRef.current = room;
      
      // Publish microphone
      const tracks = await createLocalTracks({ audio: true });
      await room.localParticipant.publishTrack(tracks[0]);
      
      // Setup audio monitoring
      if (tracks[0].mediaStreamTrack) {
        setupAudioLevelMonitoring(tracks[0].mediaStreamTrack);
      }
      
      // Tell backend to join
      await fetch('http://localhost:3001/api/livekit/join-room', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ roomName })
      });
      
      // Start listening for transcriptions
      startTranscriptionMonitoring();
      
    } catch (error) {
      console.error('Connection error:', error);
    }
  };

  const startTranscriptionMonitoring = () => {
    const eventSource = new EventSource('http://localhost:3001/api/livekit/transcriptions');
    
    eventSource.onmessage = (event) => {
      const data = JSON.parse(event.data);
      
      if (data.type === 'transcription') {
        const displayLatency = Date.now() - data.audioStartTime;
        
        setTranscriptions(prev => {
          // Update interim or add new
          if (!data.isFinal && prev.length > 0 && !prev[prev.length - 1].isFinal) {
            const updated = [...prev];
            updated[updated.length - 1] = {
              text: data.text,
              latency: displayLatency,
              timestamp: Date.now(),
              isFinal: false
            };
            return updated;
          } else {
            return [...prev, {
              text: data.text,
              latency: displayLatency,
              timestamp: Date.now(),
              isFinal: data.isFinal
            }];
          }
        });
      }
    };
  };

  return (
    <div>
      <h2>Voice Input</h2>
      
      {/* Audio Level Meter */}
      <div style={{ display: 'flex', alignItems: 'center', gap: '10px' }}>
        <span>Audio Level:</span>
        <div style={{ 
          width: '200px', 
          height: '20px', 
          backgroundColor: '#f0f0f0',
          borderRadius: '10px',
          overflow: 'hidden'
        }}>
          <div style={{ 
            width: `${audioLevel}%`, 
            height: '100%', 
            backgroundColor: audioLevel > 70 ? '#ff4444' : audioLevel > 30 ? '#ffaa44' : '#44ff44',
            transition: 'width 0.1s'
          }} />
        </div>
      </div>

      <button onClick={connectToLiveKit}>Start Voice Input</button>

      {/* Transcriptions */}
      <div>
        {transcriptions.map((t, i) => (
          <div key={i}>
            <span style={{ opacity: t.isFinal ? 1 : 0.7 }}>
              {t.text}
            </span>
            <span style={{ 
              color: t.latency < 500 ? 'green' : t.latency < 1000 ? 'orange' : 'red' 
            }}>
              {t.latency}ms
            </span>
          </div>
        ))}
      </div>
    </div>
  );
};
```

## Integration into Applications

### 1. Populating Input Fields

To integrate this into an application where you need to populate input fields:

```tsx
// Example: Auto-filling a form input
const MyForm: React.FC = () => {
  const [inputValue, setInputValue] = useState('');
  
  // ... LiveKit setup code ...

  const startTranscriptionMonitoring = () => {
    const eventSource = new EventSource('http://localhost:3001/api/livekit/transcriptions');
    
    eventSource.onmessage = (event) => {
      const data = JSON.parse(event.data);
      
      if (data.type === 'transcription' && data.isFinal) {
        // Append to input
        setInputValue(prev => prev + ' ' + data.text);
        
        // Or replace entirely
        // setInputValue(data.text);
        
        // Or trigger form submission
        // if (data.text.toLowerCase().includes('submit')) {
        //   handleSubmit();
        // }
      }
    };
  };

  return (
    <form>
      <input 
        type="text" 
        value={inputValue}
        onChange={(e) => setInputValue(e.target.value)}
      />
      <button type="button" onClick={startVoiceInput}>
        ðŸŽ¤ Voice Input
      </button>
    </form>
  );
};
```

### 2. Voice Commands

```tsx
// Example: Voice command processor
const processVoiceCommand = (transcript: string) => {
  const command = transcript.toLowerCase().trim();
  
  // Navigation commands
  if (command.includes('go to')) {
    const page = command.replace('go to', '').trim();
    navigate(`/${page}`);
  }
  
  // Form commands
  else if (command.includes('clear')) {
    clearForm();
  }
  
  // Action commands
  else if (command.includes('save')) {
    saveData();
  }
  
  // Default: populate input
  else {
    updateInputField(transcript);
  }
};
```

## Performance Optimizations

### 1. Audio Processing (10ms chunks)
- Reduces latency from 100ms to 10ms per processing cycle
- Buffer size: 960 bytes (480 samples Ã— 2 bytes)

### 2. Deepgram Configuration
- Model: `nova` (balanced speed/accuracy)
- Interim results: Enabled for faster feedback
- No endpointing: Reduces processing delays

### 3. Voice Activity Detection
- RMS threshold: 500 (tuned for speech)
- Silence detection: 20ms (2 buffers)
- Per-utterance latency tracking

### 4. Network Optimization
- Server-Sent Events for low-latency updates
- KeepAlive on Deepgram WebSocket
- Direct audio streaming without buffering

## Latency Breakdown

Typical latency: **400-600ms** end-to-end

- Audio capture: ~10ms
- LiveKit transmission: ~50ms
- Backend processing: ~10ms
- Deepgram transcription: ~300-400ms
- SSE transmission: ~20ms
- Frontend rendering: ~10ms

## Error Handling

### Connection Failures
```typescript
// Retry logic for Deepgram
let retries = 0;
const maxRetries = 3;

const connectWithRetry = async () => {
  try {
    await deepgramService.startTranscription();
  } catch (error) {
    if (retries < maxRetries) {
      retries++;
      setTimeout(connectWithRetry, 1000 * retries);
    } else {
      console.error('Failed to connect after retries');
    }
  }
};
```

### Audio Quality Issues
```typescript
// Monitor audio quality
const checkAudioQuality = (buffer: Buffer) => {
  const rms = calculateRMS(buffer);
  
  if (rms < 100) {
    console.warn('Low audio level detected');
    // Prompt user to speak louder
  }
  
  if (rms > 30000) {
    console.warn('Audio clipping detected');
    // Reduce gain
  }
};
```

## Security Considerations

1. **API Key Protection**
   - Never expose API keys in frontend
   - Use environment variables
   - Implement token refresh logic

2. **Access Control**
   - Validate room access on backend
   - Implement user authentication
   - Set appropriate LiveKit grants

3. **Rate Limiting**
   - Limit token generation requests
   - Monitor Deepgram usage
   - Implement cost controls

## Deployment Checklist

- [ ] Environment variables configured
- [ ] HTTPS enabled (required for WebRTC)
- [ ] CORS configured for production domains
- [ ] LiveKit webhook endpoint secured
- [ ] Deepgram usage monitoring setup
- [ ] Error tracking (Sentry, etc.)
- [ ] Performance monitoring
- [ ] Backup audio recording (optional)

## Troubleshooting

### No Audio Detected
1. Check microphone permissions
2. Verify LiveKit track is published
3. Check VAD threshold settings

### High Latency
1. Use closer LiveKit/Deepgram regions
2. Reduce audio buffer size
3. Enable Deepgram interim results
4. Check network conditions

### Transcription Errors
1. Verify Deepgram API key
2. Check audio format (must be PCM16)
3. Monitor Deepgram connection state
4. Review audio quality

## Conclusion

This implementation provides a robust, low-latency voice input system suitable for real-time applications. The modular architecture allows easy integration into existing applications while maintaining performance and reliability.